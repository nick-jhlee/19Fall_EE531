\chapter{Introduction}

\section{Motivation}
GNN, short for Graph Neural Network, has revolutionized the field of representation learning for graph-structured datas. Scarselli {\it et al.}\cite{Scarselli2009a} developed the (probably) earliest GNN model based on information diffusion mechanism.
Since then, numerous variants of GNN models have come out, and the reader is referred to a recent survey of GNNs by Wu {\it et al.}\cite{Wu2019}.

Even though recent advances in GNNs have achieved state-of-the-art performance in many tasks such as node classification, link prediction, graph classification...etc., their designing have mostly been based upon empirical intuition, heuristics, and experimental trial-and-error.
Therefore, there isn't much theoretical understanding of the properties and limitations of GNNs, and formal analysis of GNNs' representational capacity is limited.\cite{Xu2019}
Precisely, this work presents a theoretical framework for analyzing the representational power of GNNs i.e. how expressive different GNN variants are in learning to represent and distinguish between different graph structures.


\section{Previous / Related Works}

There hasn't been much work regarding this topic of theoretical interest.
In his other work, Scarselli et al.\cite{Scarselli2009b} showed that his GNN model\cite{Scarselli2009a} can approximate measurable functions in probability. (Theorem 2-4 in his paper)

Lei et al.\cite{Lei2017} showed that their architecture lies in the RKHS(reproducing kernel Hilbert space) of graph kernels, but do not study explicitly which graph it can distinguish.


These works focus on a specific architecture and {\bf do not easily generalize to other architectures}.